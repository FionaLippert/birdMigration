#!/bin/bash

#SBATCH --partition=gpu_shared
#SBATCH --gres=gpu:1
#SBATCH --job-name=innerCV
#SBATCH --time=120:00:00
#SBATCH --output=slurm_output_%A_%a.out
#SBATCH --mail-type=BEGIN,END
#SBATCH --mail-user=f.lippert@uva.nl

# Note: use --partition=gpu_shared if not all GPUs on a node are needed, --gpus=2 would give 2 of them
# Note: use --partition=gpu_short for debugging


source activate birds

module load 2020
module load CUDA/11.0.2-GCC-9.3.0
module load cuDNN/8.0.3.33-gcccuda-2020a
module load NCCL/2.7.8-gcccuda-2020a

# read command line arguments
HPARAM_FILE=$1
MODEL=$2
TEST_YEAR=$3

JOB_FILE=$HOME/birdMigration/scripts/run_inner_cv.job
CHECKPOINTDIR=$HOME/birdMigration/checkpoints/nested_cv_$MODEL/test_${TEST_YEAR}/hp_grid_search # ${SLURM_ARRAY_JOB_ID}

mkdir -p $CHECKPOINTDIR
rsync $JOB_FILE $CHECKPOINTDIR/
rsync $HPARAM_FILE $CHECKPOINTDIR/


#Copy all necessary input files to scratch
mkdir "$TMPDIR"/data
cp -r $HOME/birdMigration/data/preprocessed "$TMPDIR"/data

cd $HOME/birdMigration/scripts

# run array jobs for all hyperparameter settings
srun python run_2.py root=$TMPDIR +sub_dir=setting_$SLURM_ARRAY_TASK_ID \
	action=cv \
	datasource.test_year=$TEST_YEAR \
	job_id=$SLURM_ARRAY_TASK_ID \
	model=$MODEL \
	experiment=hp_grid_search \
	output_dir=$CHECKPOINTDIR \
	$(head -$SLURM_ARRAY_TASK_ID $HPARAM_FILE | tail -1)

# after all jobs are done, find setting with best average validation error
python determine_best_hp.py $CHECKPOINTDIR best_hp_settings.txt



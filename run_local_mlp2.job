#!/bin/bash

#SBATCH --partition=gpu_shared
#SBATCH --gres=gpu:1
#SBATCH --job-name=LMLP2
#SBATCH --time=120:00:00
# #SBATCH --mem=16G
#SBATCH --array=1-1
#SBATCH --output=slurm_output_%A_%a.out
#SBATCH --mail-type=BEGIN,END
#SBATCH --mail-user=f.lippert@uva.nl

# Note: use --partition=gpu_shared if not all GPUs on a node are needed, --gpus=2 would give 2 of them
# Note: use --partition=gpu_short for debugging
# Note: use --gres=gpu:titanv:1 to get a titan gpu

# Activate environment
source activate birds

module load 2020
module load CUDA/11.0.2-GCC-9.3.0
module load cuDNN/8.0.3.33-gcccuda-2020a
module load NCCL/2.7.8-gcccuda-2020a

JOBFILE=$HOME/birdMigration/run_local_mlp2.job
CHECKPOINTDIR=$HOME/birdMigration/checkpoints/local_mlp/array_job_${SLURM_ARRAY_JOB_ID}
mkdir -p $CHECKPOINTDIR
cp $JOBFILE $CHECKPOINTDIR


#Copy all necessary input files to scratch
mkdir "$TMPDIR"/data
cp -r $HOME/birdMigration/data/preprocessed "$TMPDIR"/data

cd $HOME/birdMigration/scripts

srun python run.py action=train+test device.root=$TMPDIR +sub_dir=job_$SLURM_ARRAY_TASK_ID job_id=$SLURM_ARRAY_TASK_ID model=LocalMLP output_dir=$CHECKPOINTDIR


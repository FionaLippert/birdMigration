#!/bin/bash

#SBATCH --partition=gpu_shared
#SBATCH --gres=gpu:1
#SBATCH --job-name=TrainBGNN2
#SBATCH --time=120:00:00
# #SBATCH --mem=16G
#SBATCH --array=1-3
#SBATCH --output=slurm_output_%A_%a.out
#SBATCH --mail-type=BEGIN,END
#SBATCH --mail-user=f.lippert@uva.nl

# Note: use --partition=gpu_shared if not all GPUs on a node are needed, --gpus=2 would give 2 of them
# Note: use --partition=gpu_short for debugging
# Note: use --gres=gpu:titanv:1 to get a titan gpu


# Activate your environment
source activate birds


##source activate birdmigration
#module purge
##module load 2019
module load 2020
##module load Python/3.8.2-GCCcore-9.3.0
##module load Python/3.7.5-foss-2019b
##module load Anaconda3/2020.02
##module load Anaconda3/2018.12
module load CUDA/11.0.2-GCC-9.3.0
##module load CUDA/10.1.243-GCC-8.3.0
module load cuDNN/8.0.3.33-gcccuda-2020a
##module load cuDNN/7.6.5.32-gcccuda-2019b
module load NCCL/2.7.8-gcccuda-2020a
##module load NCCL/2.5.6-CUDA-10.1.243
#module load GDAL/3.0.4-foss-2020a-Python-3.8.2
##module load GDAL/2.2.3-intel-2019b-Python-3.6.6

module load parallel

JOBFILE=$HOME/birdMigration/run_boundary_birdflux2.job
CHECKPOINTDIR=$HOME/birdMigration/checkpoints/boundary_extrapolation_birdflux/array_job_${SLURM_ARRAY_JOB_ID}
mkdir -p $CHECKPOINTDIR
rsync $JOBFILE $CHECKPOINTDIR/


#Copy all necessary input files to scratch
mkdir "$TMPDIR"/data
cp -r $HOME/birdMigration/data/preprocessed "$TMPDIR"/data

cd $HOME/birdMigration/scripts

srun python run_2.py root=$TMPDIR +sub_dir=job_$SLURM_ARRAY_TASK_ID job_id=$SLURM_ARRAY_TASK_ID model=BirdFluxGraphLSTM action=train+test model.epochs=200 use_nights=True model.lr=0.0005 model.lr_decay=200 model.lr_gamma=0.1 output_dir=$CHECKPOINTDIR +model.use_uv=True +model.n_graph_layers=0 model.batch_size=8 #data_perc=0.1

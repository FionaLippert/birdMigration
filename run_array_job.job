#!/bin/bash

#SBATCH --partition=gpu_shared
#SBATCH --gres=gpu:1
#SBATCH --job-name=TrainNN
#SBATCH --time=120:00:00
#SBATCH --output=slurm_output_%A.out
#SBATCH --mail-type=BEGIN,END
#SBATCH --mail-user=f.lippert@uva.nl

# Note: use --partition=gpu_shared if not all GPUs on a node are needed, --gpus=2 would give 2 of them
# Note: use --partition=gpu_short for debugging


# Activate your environment
source activate birds


##source activate birdmigration
#module purge
##module load 2019
module load 2020
##module load Python/3.8.2-GCCcore-9.3.0
##module load Python/3.7.5-foss-2019b
##module load Anaconda3/2020.02
##module load Anaconda3/2018.12
module load CUDA/11.0.2-GCC-9.3.0
##module load CUDA/10.1.243-GCC-8.3.0
#module load cuDNN/8.0.3.33-gcccuda-2020a
##module load cuDNN/7.6.5.32-gcccuda-2019b
#module load NCCL/2.7.8-gcccuda-2020a
##module load NCCL/2.5.6-CUDA-10.1.243
#module load GDAL/3.0.4-foss-2020a-Python-3.8.2
##module load GDAL/2.2.3-intel-2019b-Python-3.6.6




#Copy all necessary input files to scratch
mkdir "$TMPDIR"/data
cp -r $HOME/birdMigration/data/preprocessed "$TMPDIR"/data


JOB_FILE=$HOME/birdMigration/array_job.job
HPARAMS_FILE=$HOME/birdMigration/array_job_hyperparameters.txt
CHECKPOINTDIR=$HOME/birdMigration/checkpoints/array_job_${SLURM_ARRAY_JOB_ID}

mkdir $CHECKPOINTDIR
rsync $HPARAMS_FILE $CHECKPOINTDIR/
rsync $JOB_FILE $CHECKPOINTDIR/

# Run your code
srun python -u train.py \
               --checkpoint_path $CHECKPOINTDIR/experiment_${SLURM_ARRAY_TASK_ID} \
               $(head -$SLURM_ARRAY_TASK_ID $HPARAMS_FILE | tail -1)


cd $HOME/birdMigration/scripts
# Run your code
srun python -u run.py root="$TMPDIR" model=LocalLSTM edge_type=none birds_per_km2=True experiment=encoder_no_nights model.epochs=5 repeats=1 data_perc=0.1 use_nights=False data_parallel=False
#srun python -u test_script.py root="$TMPDIR"

#Copy output directory from scratch to home
mkdir "$HOME"/output_"$SLURM_JOBID"
cp -r "$TMPDIR"/results $HOME/birdMigration/output_"$SLURM_JOBID"
